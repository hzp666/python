import requests
from bs4 import BeautifulSoup
import jieba
from collections import Counter
import jieba.posseg


# the method about download the source code or content
# return list
def crawel(url):

    # split_title to save the title's cut words
    split_title = []

    # get the HTML'S content
    source_code = requests.get(url).text
    soup = BeautifulSoup(source_code, 'html.parser')
    for tag in soup.select('.title'):
        title = tag.string

        # split the HTML's content
        split_title.append(jieba.lcut(title))
    return split_title


# the method for clean the source or content,especially the punctuation
# return list
def clean_word(each_words):
    # final list , to transfer two dimension to one dimension
    final_list = []

    # list for save the useless Symbol
    string_dic = ['+', '-', '*', '/', '!', '@', '@', '#', '$', '%', '^', ':',
                  '&', '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}',
                  '?', ',', '.', '/', '', '-', '——', '。', '；', '：', ' ', '—',
                  '（', '）', '|', ' ', '，', '! ', '！', '-', '：', '，', '“', '”',
                  ' ', '-']
    # transfer the useless symbol to ' '
    for i in range(0, len(each_words)):
        for m in range(0, len(each_words[i])):
            if each_words[i][m] in string_dic:
                each_words[i][m] = ' '

    # delete the list's ' ' which replace step remained
    for i in range(0, len(each_words)):
        # try for the list's function Index(),if it don't find the elements then Except
        try:
            for x in range(0, len(each_words[i])):
                del each_words[i][each_words[i].index(' ')]

        except ValueError:
            continue

    # final list , transfer two dimension to one dimension
    for i in range(0, len(each_words)):
        for x in range(0, len(each_words[i])):
            final_list.append(each_words[i][x])

    return final_list


# method about return the words and the part of speech
# return list
def part_of_speech(url):
    # split_title to save the title's cut words
    split_title = []

    # final_list for return
    final_list = []

    # get the HTML'S content
    source_code = requests.get(url).text
    soup = BeautifulSoup(source_code, 'html.parser')
    for tag in soup.select('.title'):
        title = tag.string

        # split the HTML's content
        split_title.append(jieba.posseg.lcut(title))

# clean
    # list for save the useless Symbol
    string_dic = ['+', '-', '*', '/', '!', '@', '@', '#', '$', '%', '^', ':',
                  '&', '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}',
                  '?', ',', '.', '/', '', '-', '——', '。', '；', '：', ' ', '—',
                  '（', '）', '|', ' ', '，', '! ', '！', '-', '：', '，', '“', '”',
                  ' ', '-', '？', '】', '【']
    # final list , transfer two dimension to one dimension
    for i in range(0, len(split_title)):
        for x in range(0, len(split_title[i])):
            final_list.append(split_title[i][x])

    # transfer the useless symbol to ' '
    for x in range(0, len(final_list)):
        if final_list[x].word in string_dic:
            final_list[x] = ' '

    # delete the list's ' ' which replace step remained

    for x in range(0, len(final_list)):
        try:
            del final_list[final_list.index(' ')]
        except ValueError:
            continue

    return final_list



# use self dictionary 
txt = u'欧阳建国是创新办主任也是欢聚时代公司云计算方面的专家'
jieba.load_userdict('book.txt')
# book.txt's content below
# 'word' 'frequency' 'part of the speech'
# 欧阳建国 5 n
# 拆迁办 5 n
# 云计算 5 n
# 欢聚时代 5 n
# 创新办 5 n

print(jieba.lcut(txt))


url = 'http://geek.csdn.net/'

# func1 = crawel(url)
# print(func1)

# func2 = clean_word(func1)
# print(func2)


# # counter the list's words frequency
# c = Counter(func2).most_common(20)
# print(c)

func3 = part_of_speech(url)
print(func3)
